<!DOCTYPE html>
<html>

  <head>
    <meta charset='utf-8'>
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <meta name="description" content="Hadoop-Components : Hadoop Components">

    <link rel="stylesheet" type="text/css" media="screen" href="stylesheets/stylesheet.css">

    <title>Hadoop Components</title>
  </head>

  <body>

    <!-- HEADER -->
    <div id="header_wrap" class="outer">
        <header class="inner">
        </header>
    </div>

    <!-- MAIN CONTENT -->
    <div id="main_content_wrap" class="outer">
      <section id="main_content" class="inner">
        <p><img src="http://social.technet.microsoft.com/wiki/cfs-file.ashx/__key/communityserver-wikis-components-files/00-00-00-00-05/7848.TheHadoopEcosystem.png?raw=true?raw=true" alt="Hadoop Ecosystem"></p>

<p><strong>HDFS</strong> - Stores data on the cluster.</p>

<p><strong>MapReduce</strong> - Processes data on the cluster. </p>

<h3>
<a id="hadoop-cluster" class="anchor" href="#hadoop-cluster" aria-hidden="true"><span class="octicon octicon-link"></span></a>Hadoop Cluster</h3>

<p>A group of machines working together to store and process data. It can have any number of "slave" or "worker" nodes with 2 master nodes:</p>

<ol>
<li>Name Node - Manages HDFS.</li>
<li>Job Tracker - Manages MapReduce.</li>
</ol>

<p><img src="http://www.ibm.com/developerworks/library/bd-hadoopyarn/figure1.png" alt="Hadoop Cluster"></p>

<h2>
<a id="hdfs" class="anchor" href="#hdfs" aria-hidden="true"><span class="octicon octicon-link"></span></a>HDFS?</h2>

<ol>
<li>HDFS is a filesystem written in Java.</li>
<li>It's based on Google File System.</li>
<li>Sits on native filesystem such as ext3, ext4 or xfs.</li>
<li>Provides redundant storage for massive amounts of data. </li>
<li>It performs best with a "modest" number of large files. Each file typically 100MB or more.</li>
<li>Files in HDFS are "write once" - No random rights to file are allowed.</li>
<li>It is optimized for large, streaming reads of files, rather than random reads.</li>
</ol>

<h3>
<a id="how-files-are-stored" class="anchor" href="#how-files-are-stored" aria-hidden="true"><span class="octicon octicon-link"></span></a>How files are stored?</h3>

<p>Each data file is splitted into block on multiple nodes at data load time. Each block is replicated on multiple nodes (DataNode) default 3x and NameNode is used to store metadata.</p>

<p><img src="http://bradhedlund.s3.amazonaws.com/2011/hadoop-network-intro/Writing-Files-to-HDFS.PNG" alt="Writing File to HDFS"></p>

<h3>
<a id="hdfs-namenode-availability" class="anchor" href="#hdfs-namenode-availability" aria-hidden="true"><span class="octicon octicon-link"></span></a>HDFS NameNode availability</h3>

<p>NameNode daemon must be running at all times. If NameNode stops, cluster become inaccessible.</p>

<h3>
<a id="how-to-access-hdfs" class="anchor" href="#how-to-access-hdfs" aria-hidden="true"><span class="octicon octicon-link"></span></a>How to access HDFS?</h3>

<ul>
<li>FsShell Command line : hadoop fs</li>
<li>JAVA API</li>
<li>Ecosystem Projects
Flume -  Collect data from network sources e.g System logs.
Sqoop -  Transfers data between HDFS and RDBMS.
Hue - Web based interactive UI. Can"browse,"upload," download,"and"view"files".</li>
</ul>

<h2>
<a id="mapreduce" class="anchor" href="#mapreduce" aria-hidden="true"><span class="octicon octicon-link"></span></a>MapReduce</h2>

<p>MapReduce is a method of distributing a task over multiple nodes. Each node is responsible to process data stored on that node. It consist of 2 phases:</p>

<ol>
<li>Map</li>
<li>Reduce</li>
</ol>

<h3>
<a id="features-of-mapreduce" class="anchor" href="#features-of-mapreduce" aria-hidden="true"><span class="octicon octicon-link"></span></a>Features of MapReduce</h3>

<ol>
<li>Automatic parallelization and distribution.</li>
<li>Fault tolerance.</li>
<li>A clean abstraction for programmers. MapReduce "programs" are usually written in Java but can be written in any language 
using <strong>"Hadoop Streaming"</strong>. All of Hadoop is written in Java.</li>
<li>It makes Developer can simply concentrate on writing the "Map" and "Reduce" functions.</li>
</ol>

<h3>
<a id="mapreduce-stages" class="anchor" href="#mapreduce-stages" aria-hidden="true"><span class="octicon octicon-link"></span></a>MapReduce Stages</h3>

<p><img src="https://encrypted-tbn1.gstatic.com/images?q=tbn:ANd9GcQmycmzO1vsDSut2tZvZ4v9UIjjJ7B5dr4-f9E1yXUqnH6EpdiwjQ" alt="MapReduce Stages"></p>

<p><strong>The Mapper</strong> - Often used to parse, filter, or transform the data.</p>

<p>-Input: "key/value" pair.</p>

<p>-Output: A list of zero or more "key/value" pairs.</p>

<ol>
<li>Each Map task (typically) operates on a single "HDFS" block.</li>
<li>Map tasks (usually) run on the node where the block of file or data is stored.</li>
</ol>

<p><strong>Shuffle and Sort</strong></p>

<ol>
<li>Sorts and consolidates intermediate data from all mappers.</li>
<li>Happens after all Map tasks are complete and before Reduce tasks start. </li>
</ol>

<p><strong>The Reducer</strong> - Often aggregate data using statistical functions.</p>

<p>-Input: Output of a Mapper.</p>

<p>-Output: Zero or more final "key/value" pairs. These are written to HDFS.</p>

<ol>
<li>Operates on shuffled/sorted intermediate data Map task output.</li>
<li>Produces final output.</li>
</ol>
      </section>
    </div>

  </body>
</html>
