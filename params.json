{"name":"Hadoop - An Elephant","tagline":"Hadoop","body":"## What is Hadoop?\r\nApache Hadoop is a set of algorithms (an open-source software framework written in Java) for distributed storage and distributed processing of very large data sets (Big Data) on computer clusters built from commodity hardware. All the modules in Hadoop are designed with a fundamental assumption that hardware failures (of individual machines, or racks of machines) are commonplace and thus should be automatically handled in software by the framework.\r\n\r\nThe core of Apache Hadoop consists of a storage part (Hadoop Distributed File System (HDFS)) and a processing part (MapReduce). \r\n\r\nHadoop splits files into large blocks (default 64MB or 128MB) and distributes the blocks amongst the nodes in the cluster. To process the data, Hadoop Map/Reduce transfers code (specifically Jar files) to nodes that have the required data, which the nodes then process in parallel. This approach takes advantage of data locality to allow the data to be processed faster and more efficiently via distributed processing than by using a more conventional supercomputer architecture that relies on a parallel file system where computation and data are connected via high-speed networking.\r\n\r\nThe base Apache Hadoop framework is composed of the following modules:\r\n\r\n* **Hadoop Common** – contains libraries and utilities needed by other Hadoop modules.\r\n* **Hadoop Distributed File System (HDFS)** – a distributed file-system that stores data on commodity machines, providing very high aggregate bandwidth across the cluster.\r\n* **Hadoop YARN** – a resource-management platform responsible for managing compute resources in clusters and using them for scheduling of users' applications.\r\n* **Hadoop MapReduce** – a programming model for large scale data processing.","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}